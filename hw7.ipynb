{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a16191b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lance Uy - 1006123570\n",
    "\n",
    "# Homework 8: (a) Posterior Predictive Distributions<br> and (b) Missing Data Imputation\n",
    "\n",
    "### 1. Describe how the posterior predictive distribution is created for mixture models \n",
    "\n",
    "1. **Parameter Estimation:** Estimate the parameters of the mixture model, such as means, variances, and mixing coefficients, from the observed data.\n",
    "\n",
    "2. **Posterior Distribution:** Compute the posterior distribution of these parameters given the observed data using Bayes' theorem. This involves considering the likelihood of the data given the parameters and incorporating prior beliefs about the parameters.\n",
    "\n",
    "3. **Predictive Distribution for New Data:** Integrate over all possible values of the parameters to obtain the predictive distribution for a new data point. This involves considering the mixture components and their associated probabilities.\n",
    "\n",
    "4. **Account for Mixture Components:** For each mixture component, calculate the likelihood of the new data point and take a weighted average according to the mixing coefficients. This accounts for the uncertainty in the mixture model and the contribution of each component to the predictive distribution.\n",
    "\n",
    "5. **Sampling Methods:** If the integral cannot be computed analytically, use sampling methods such as MCMC to approximate the posterior predictive distribution. This involves sampling from the posterior distribution of the parameters and using these samples to generate predictive samples.\n",
    "\n",
    "\n",
    "### 2. Describe how the posterior predictive distribution is created in general\n",
    "\n",
    "In Bayesian statistics, the posterior predictive distribution is a fundamental concept that combines prior knowledge with observed data to make predictions about future or unobserved data. The posterior predictive distribution is essentially the distribution of new data points, given both the observed data and prior beliefs.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Define a Bayesian Model**: Start with a probabilistic model that describes the relationship between the observed data, parameters of interest, and any prior information. This model typically consists of a likelihood function representing the data generation process and prior distributions for the parameters.\n",
    "\n",
    "2. **Obtain the Posterior Distribution**: Use Bayes' theorem to update the prior beliefs with the observed data, obtaining the posterior distribution of the parameters. Mathematically, this is expressed as: $p(θ|X) = \\frac{p(X|θ) p(θ)}{p(X)}$\n",
    "\n",
    "3. **Predictive Distribution**: Once the posterior distribution of the parameters is obtained, you can then use it to predict new data points. This is done by integrating over all possible parameter values weighted by their posterior probabilities. Mathematically, the posterior predictive distribution $( p(x_{\\text{new}}|D) )$ for a new data point $( x_{\\text{new}} )$ is given by: $p(x_{\\text{new}}|X) = ∫ p(x_{\\text{new}}|θ) * p(θ|X) dθ$ where $( p(x_{\\text{new}}|θ) )$ is the likelihood of the new data point given the parameter values.\n",
    "\n",
    "4. **Sampling Methods**: In practice, calculating the posterior predictive distribution analytically may be intractable for complex models. In such cases, sampling methods like Markov Chain Monte Carlo (MCMC) or Variational Inference are often used to approximate the posterior distribution. Once samples from the posterior distribution are obtained, they can be used to approximate the predictive distribution by sampling from the likelihood function using these parameter samples.\n",
    "\n",
    "5. **Evaluate Predictions**: Finally, the posterior predictive distribution allows you to make predictions about future observations. You can assess the uncertainty of these predictions by examining the spread of the predictive distribution. Additionally, you can compare predictions to observed data to evaluate the model's performance and refine the model if necessary.\n",
    "\n",
    "\n",
    "### 3. Have glance through [this](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html) and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
    "\n",
    "- **Hint: latent variables $v$ indicating the subpopulation are competely missing values that we simply treat as paramters to be inferred though posterior analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...**\n",
    "\n",
    "In the scenario where we're conducting a regression analysis with \\( y \\) as the dependent variable and \\( X \\) containing some missing values, Bayesian analysis offers a flexible approach to handle this issue without discarding incomplete rows. Instead of excluding incomplete cases, Bayesian methods incorporate latent variables to represent missing data, treating them as parameters to be estimated.\n",
    "\n",
    "1. **Introducing Latent Variables**: We introduce latent variables, denoted as \\( v \\), to signify the subpopulation for which data are missing. These latent variables serve as indicators of the presence or absence of missing values in \\( X \\).\n",
    "\n",
    "2. **Model Specification**: Our regression model is extended to include parameters related to the latent variables representing missing data. For instance, if \\( X \\) has missing values, we augment the model to incorporate parameters capturing the relationship between the probability of missingness and other observed variables.\n",
    "\n",
    "3. **Bayesian Inference**: We employ Bayesian inference techniques to estimate the posterior distribution of model parameters, including those associated with the latent variables representing missing data. This involves updating our prior beliefs based on the observed data to derive the posterior distribution.\n",
    "\n",
    "4. **Imputation of Missing Values**: With the posterior distribution of parameters in hand, we proceed to impute missing values in \\( X \\). This is achieved by generating multiple imputations using samples from the posterior predictive distribution of the missing variables, given the observed data and estimated parameters.\n",
    "\n",
    "5. **Model Evaluation and Sensitivity Analysis**: We assess the imputed values and evaluate the sensitivity of our results to the missing data mechanism. It's crucial to consider the assumption of Missing Completely At Random (MCAR) and explore the robustness of our findings to different assumptions about the missing data process.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
